{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fbeb3f",
   "metadata": {},
   "source": [
    "### 1.安装依赖包并导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbee1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk==3.5 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install tqdm==4.62.3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install openpyxl==3.0.7 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install scikit_learn==0.23.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install pandas==1.1.5 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install numpy==1.19.5 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80cf4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from string import punctuation\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ad3b2",
   "metadata": {},
   "source": [
    "### 2. 读取停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0bfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b76a7",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 移除标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37914d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    # 移除标点符号\n",
    "    punctuation_zh = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”？，！【】（）、。：；’‘……￥·\"\"\"\n",
    "    dicts = {i: '' for i in punctuation + punctuation_zh}\n",
    "    punc_table = str.maketrans(dicts)\n",
    "    new_text = text.translate(punc_table)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7849b1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>必选关键词</th>\n",
       "      <th>正向关键词</th>\n",
       "      <th>负向关键词</th>\n",
       "      <th>文件名</th>\n",
       "      <th>页数</th>\n",
       "      <th>标签</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Considering  the  regular  pore  size, stabili...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>parameter</td>\n",
       "      <td>reaction</td>\n",
       "      <td>139.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conditions, After several rounds of parameter ...</td>\n",
       "      <td>optimization</td>\n",
       "      <td>parameter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>171.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supplementary Figure 78: Localization of the L...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>cell</td>\n",
       "      <td>LUMO</td>\n",
       "      <td>15-SI.pdf</td>\n",
       "      <td>52</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Supplementary Figure 77: Localization of the H...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>cell</td>\n",
       "      <td>HOMO</td>\n",
       "      <td>15-SI.pdf</td>\n",
       "      <td>51</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Materials and characterization    Optimization...</td>\n",
       "      <td>optimization</td>\n",
       "      <td>cell</td>\n",
       "      <td>adsorption</td>\n",
       "      <td>153-SI.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>the three COFs adopt AA eclipsed stacking in t...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>520.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>Density functional theory (DFT) calculation wa...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>520.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>Figure 3, The optimized binding sites and bind...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>520.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>which is because the 6FDA-ODA matrix is still ...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>520.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>The thickness of m-Naph-COF film was measured ...</td>\n",
       "      <td>optimize</td>\n",
       "      <td>XRD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>522-SI.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1790 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text         必选关键词  \\\n",
       "0     Considering  the  regular  pore  size, stabili...      optimize   \n",
       "1     conditions, After several rounds of parameter ...  optimization   \n",
       "2     Supplementary Figure 78: Localization of the L...      optimize   \n",
       "3     Supplementary Figure 77: Localization of the H...      optimize   \n",
       "4     Materials and characterization    Optimization...  optimization   \n",
       "...                                                 ...           ...   \n",
       "1785  the three COFs adopt AA eclipsed stacking in t...      optimize   \n",
       "1786  Density functional theory (DFT) calculation wa...      optimize   \n",
       "1787  Figure 3, The optimized binding sites and bind...      optimize   \n",
       "1788  which is because the 6FDA-ODA matrix is still ...      optimize   \n",
       "1789  The thickness of m-Naph-COF film was measured ...      optimize   \n",
       "\n",
       "          正向关键词       负向关键词         文件名  页数   标签 Unnamed: 7  \n",
       "0     parameter    reaction     139.pdf   5   no        NaN  \n",
       "1     parameter         NaN     171.pdf   5   no        NaN  \n",
       "2          cell        LUMO   15-SI.pdf  52   no        NaN  \n",
       "3          cell        HOMO   15-SI.pdf  51   no        NaN  \n",
       "4          cell  adsorption  153-SI.pdf   2   no        NaN  \n",
       "...         ...         ...         ...  ..  ...        ...  \n",
       "1785        NaN         NaN     520.pdf   2  NaN        NaN  \n",
       "1786        NaN         NaN     520.pdf   3  NaN        NaN  \n",
       "1787        NaN         NaN     520.pdf   3  NaN        NaN  \n",
       "1788        NaN         NaN     520.pdf   5  NaN        NaN  \n",
       "1789        XRD         NaN  522-SI.pdf   3  NaN        NaN  \n",
       "\n",
       "[1790 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel('data-标签.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474b816",
   "metadata": {},
   "source": [
    "### 4. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41309f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    max_len = 0\n",
    "    data = pd.read_excel(path).dropna(subset=['标签'])\n",
    "    for idx, row in data.iterrows():\n",
    "        temp = []\n",
    "        line = str(row['text']).strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        line = remove_punc(line)\n",
    "        text = [j for j in word_tokenize(line) if j not in stop_words]\n",
    "        if len(text)==0:\n",
    "            continue\n",
    "        label = row['标签']\n",
    "        texts.append(' '.join(text))\n",
    "        max_len = max(max_len, len(text))\n",
    "        labels.append(label)\n",
    "    assert len(texts) == len(labels)\n",
    "    print(max_len)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692ff568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "texts, labels = read_data(\"data-标签.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8facd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, {'no', 'yes'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(labels)),set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6781de76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474c010",
   "metadata": {},
   "source": [
    "### 5. 提取BOW特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c0306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_feature(data, flag='rf'):\n",
    "    if os.path.exists(flag + '_tokenizer.pkl'):\n",
    "        print('Tokenizer founded. Loading...')\n",
    "        with open(flag + '_tokenizer.pkl', 'rb') as f:\n",
    "            transformer = pickle.load(f)\n",
    "    else:\n",
    "        print('No Tokenizer founded. Creating...')\n",
    "        transformer = CountVectorizer(max_features=2000)\n",
    "        transformer.fit(data)\n",
    "        # 保存装换器\n",
    "        with open(flag + '_tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(transformer, f)\n",
    "    tfidf = transformer.transform(data)\n",
    "    return tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011baee",
   "metadata": {},
   "source": [
    "### 6. 计算评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d767199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels, preds):\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    # precision\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    # recall\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "    # f1_score\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    print(f'accuracy: {accuracy}')\n",
    "    print(f'precision: {precision}')\n",
    "    print(f'recall: {recall}')\n",
    "    print(f'f1: {f1}')\n",
    "    print(f'classification_report: ')\n",
    "    print(classification_report(labels, preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e53b3e",
   "metadata": {},
   "source": [
    "### 7. 寻找模型最优超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4322a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyperparam(train_x, train_y):\n",
    "    f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    scoring = {'F1': f1_scorer, 'Accuracy': acc_scorer}\n",
    "    params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
    "    }\n",
    "    model = RandomForestClassifier()\n",
    "    # 交叉验证，GridSearch搜索最优参数\n",
    "    model = GridSearchCV(model,\n",
    "                         param_grid=params,\n",
    "                         scoring=scoring,\n",
    "                         refit=\"Accuracy\",\n",
    "                         return_train_score=True,\n",
    "                         cv=5,\n",
    "                         verbose=1,\n",
    "                         n_jobs=-1)\n",
    "    model.fit(train_x, train_y)\n",
    "    # performance metrics\n",
    "    print(model.best_score_, model.best_params_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f32b6",
   "metadata": {},
   "source": [
    "### 8. 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "622fc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和测试\n",
    "def train_and_eval(train_x, train_y, test_x, test_y, flag, need_recover=True):\n",
    "    # 训练模型\n",
    "    if need_recover:\n",
    "        if os.path.exists(flag + '_model.pkl'):\n",
    "            print('Model founded. Loading...')\n",
    "            with open(flag + '_model.pkl', 'rb') as f:\n",
    "                clf = pickle.load(f)\n",
    "        else:\n",
    "            print('No model founded. Creating...')\n",
    "            clf = get_best_hyperparam(train_x, train_y)\n",
    "            with open(flag + '_model.pkl', 'wb') as f:\n",
    "                pickle.dump(clf, f)\n",
    "    else:\n",
    "        print('Not to recover, Creating...')\n",
    "        clf = get_best_hyperparam(train_x, train_y)\n",
    "        with open(flag + '_model.pkl', 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "    # 测试\n",
    "    print('Train:')\n",
    "    pred = clf.predict(train_x)\n",
    "    compute_metrics(train_y, pred)\n",
    "\n",
    "    print('Eval:')\n",
    "    pred = clf.predict(test_x)\n",
    "    compute_metrics(test_y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076cd59",
   "metadata": {},
   "source": [
    "### 9.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da99cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于预测\n",
    "def predict(test_text, label_encoder, flag='rf'):\n",
    "    with open(flag + '_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(flag + '_tokenizer.pkl', 'rb') as f:\n",
    "        transformer = pickle.load(f)\n",
    "    tests = []\n",
    "    for data in test_text:\n",
    "        text = remove_punc(data)\n",
    "        text = [item for item in word_tokenize(text) if item not in stop_words]\n",
    "        tests.append(' '.join(text))\n",
    "    test_x = transformer.transform(tests).toarray()\n",
    "    preds = model.predict(test_x)\n",
    "    pred_labels = []\n",
    "    for pred in preds:\n",
    "        pred_label = label_encoder.inverse_transform([pred])[0]\n",
    "        pred_labels.append(pred_label)\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636396e",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aafc708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer founded. Loading...\n"
     ]
    }
   ],
   "source": [
    "data = bow_feature(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa4e923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62b1fe",
   "metadata": {},
   "source": [
    "### Label转换为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94b5f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "label = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71eeca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c3258",
   "metadata": {},
   "source": [
    "### 训练集验证集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cc0f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data,\n",
    "                                                    label,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f692a2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f903ff84",
   "metadata": {},
   "source": [
    "### 训练以及验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "345dd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(labels, preds):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    tp = cm[1, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def train_and_eval(train_x, train_y, test_x, test_y, flag, need_recover=True):\n",
    "    if need_recover:\n",
    "        if os.path.exists(flag + '_model.pkl'):\n",
    "            print('Model founded. Loading...')\n",
    "            with open(flag + '_model.pkl', 'rb') as f:\n",
    "                clf = pickle.load(f)\n",
    "        else:\n",
    "            print('No model founded. Creating...')\n",
    "            clf = get_best_hyperparam(train_x, train_y)\n",
    "            with open(flag + '_model.pkl', 'wb') as f:\n",
    "                pickle.dump(clf, f)\n",
    "    else:\n",
    "        print('Not to recover, Creating...')\n",
    "        clf = get_best_hyperparam(train_x, train_y)\n",
    "        with open(flag + '_model.pkl', 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "\n",
    "    print('Train:')\n",
    "    pred_train = clf.predict(train_x)\n",
    "    compute_metrics(train_y, pred_train)\n",
    "    tp, tn, fp, fn = compute_confusion_matrix(train_y, pred_train)\n",
    "    print('Train - TP: {}, TN: {}, FP: {}, FN: {}'.format(tp, tn, fp, fn))\n",
    "\n",
    "    print('Eval:')\n",
    "    pred_test = clf.predict(test_x)\n",
    "    compute_metrics(test_y, pred_test)\n",
    "    tp, tn, fp, fn = compute_confusion_matrix(test_y, pred_test)\n",
    "    print('Eval - TP: {}, TN: {}, FP: {}, FN: {}'.format(tp, tn, fp, fn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d96ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not to recover, Creating...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9400000000000001 {'criterion': 'entropy', 'n_estimators': 100}\n",
      "Train:\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000       229\n",
      "           1     1.0000    1.0000    1.0000       221\n",
      "\n",
      "    accuracy                         1.0000       450\n",
      "   macro avg     1.0000    1.0000    1.0000       450\n",
      "weighted avg     1.0000    1.0000    1.0000       450\n",
      "\n",
      "Train - TP: 221, TN: 229, FP: 0, FN: 0\n",
      "Eval:\n",
      "accuracy: 0.94\n",
      "precision: 0.9416666666666667\n",
      "recall: 0.9351395730706076\n",
      "f1: 0.9379909053327822\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9048    0.9268        21\n",
      "           1     0.9333    0.9655    0.9492        29\n",
      "\n",
      "    accuracy                         0.9400        50\n",
      "   macro avg     0.9417    0.9351    0.9380        50\n",
      "weighted avg     0.9403    0.9400    0.9398        50\n",
      "\n",
      "Eval - TP: 28, TN: 19, FP: 2, FN: 1\n"
     ]
    }
   ],
   "source": [
    "def compute_confusion_matrix(labels, preds):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    tp = cm[1, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "train_and_eval(train_x, train_y, test_x, test_y, flag='rf', need_recover=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd369d0",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data-标签.xlsx')\n",
    "result = []\n",
    "for idx, row in data.iterrows():\n",
    "    temp = []\n",
    "    line = str(row['text']).strip()\n",
    "    true_label = row['标签']  # 将变量名从label更改为true_label\n",
    "    if pd.isna(true_label):\n",
    "        predicted_label = predict([line], le)[0]  # 将变量名从label更改为predicted_label\n",
    "        result.append(predicted_label)\n",
    "    else:\n",
    "        result.append(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dedbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred_label'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('result.xlsx', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5872f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfa17db95d2ae67305c40021d0f603443925e424b4da1f754f9d67cfe258cfe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
